{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2028659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from read_dataset import *\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bef6090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\z004r5cc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"An insulating film substantially free from surface irregularities is RF bias sputtered onto a smooth polycrystalline or micro-roughened surface. Controlled sputtering is performed first at a low reemission coefficient and then, after a substantially continuous layer of insulative amorphous film is deposited over the substrate, increasing the reemission coefficient to a second higher level. A low reemission coefficient is about 0.25 and a high coefficient is about 0.7.\"\n",
    "textblober = TextBlob(a)\n",
    "sentences = textblober.sentences\n",
    "len(sentences), sentences\n",
    "nltk.download('punkt')\n",
    "#nltk.word_tokenize(str(sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c51c74",
   "metadata": {},
   "source": [
    "# FabNER dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a262024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path):\n",
    "    data = pd.read_csv(path, sep = ' ', header=None, names = [\"tokens\",\"ner_tags\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2827521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fabner_train = read_files( r\"C:\\Users\\z004r5cc\\Documents\\Archieve_linuxmachine\\instructner0714\\instructner\\data\\fabNER\\S2-train.txt\")\n",
    "fabner_val = read_files(r\"C:\\Users\\z004r5cc\\Documents\\Archieve_linuxmachine\\instructner0714\\instructner\\data\\fabNER\\S3-val.txt\")\n",
    "fabner_test = read_files( r\"C:\\Users\\z004r5cc\\Documents\\Archieve_linuxmachine\\instructner0714\\instructner\\data\\fabNER\\S1-test.txt\")\n",
    "#fabner_train = read_files(\"fabNER/S2-train.txt\") #r\"C:\\Users\\z004r5cc\\Documents\\Archieve_linuxmachine\\instructner0714\\instructner\\data\\fabNER\\S2-train.txt\")\n",
    "#fabner_val = read_files(\"fabNER/S2-val.txt\") #r\"C:\\Users\\z004r5cc\\Documents\\Archieve_linuxmachine\\instructner0714\\instructner\\data\\fabNER\\S3-val.txt\")\n",
    "#fabner_test = read_files( \"fabNER/S2-test.txt\" ) #r\"C:\\Users\\z004r5cc\\Documents\\Archieve_linuxmachine\\instructner0714\\instructner\\data\\fabNER\\S1-test.txt\")\n",
    "fabner_total = pd.concat([fabner_train, fabner_val, fabner_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a59dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fabner_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0edc0b1",
   "metadata": {},
   "source": [
    "#### eliminate prefix, I B O "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe7d7451",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_jsonl_fabner_simple_with_textblob(df, dataset_path):\n",
    "    \"\"\" 1) transform the csv dataframe to json \n",
    "        2) eliminate the prefix B I O \n",
    "        3) use TextBlob to separate the sentences\"\"\"\n",
    "    \n",
    "    with open(dataset_path, 'w', encoding='utf-8') as f:\n",
    "        mask = df[\"ner_tags\"] != \"O\"\n",
    "        df.loc[mask,\"ner_tags\"] = df[mask][\"ner_tags\"].apply(lambda x: x.split(\"-\")[1])\n",
    "        dic_seq = {}\n",
    "        tokens = \"\"\n",
    "        token_list = []\n",
    "        ner_tags_index = 0\n",
    "        ner_tags_list = []\n",
    "        for index, row in df.iterrows():\n",
    "            # consider all tokens into the string context\n",
    "            tokens = tokens + row[\"tokens\"] + \" \" \n",
    "            # write all ner_tags in one long list\n",
    "            ner_tags_list.append(row[\"ner_tags\"])\n",
    "\n",
    "        #separate to sentences    \n",
    "        textblober = TextBlob(tokens)\n",
    "        sentences = textblober.sentences    \n",
    "\n",
    "        for sentence in sentences:\n",
    "            token_list = sentence.split(\" \")\n",
    "            dic_seq[\"tokens\"] = token_list\n",
    "            dic_seq[\"ner_tags\"] = ner_tags_list[ner_tags_index:(ner_tags_index+len(token_list)) ]\n",
    "            f.write(json.dumps(dic_seq) + \"\\n\")\n",
    "            # update all parameters\n",
    "            ner_tags_index = ner_tags_index + len(token_list)\n",
    "            dic_seq = {}\n",
    "            token_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fb4ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_jsonl_fabner_simple_with_textblob(fabner_train, dataset_path = \"fabNER/fabner_simple_train.jsonl\")\n",
    "get_jsonl_fabner_simple_with_textblob(fabner_val, dataset_path = \"fabNER/fabner_simple_val.jsonl\")\n",
    "get_jsonl_fabner_simple_with_textblob(fabner_test, dataset_path = \"fabNER/fabner_simple_test.jsonl\")\n",
    "get_jsonl_fabner_simple_with_textblob(fabner_total, dataset_path = \"fabNER/fabner_simple_total.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8213bcf2",
   "metadata": {},
   "source": [
    "### get original sentences, transform entites into the chatGPT output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f2b770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_sentences_from_tokens_nertags(path_in, path_out):\n",
    "    with open(path_in, 'r', encoding='utf-8') as f:\n",
    "        sentence_list = []\n",
    "        entities_list = []\n",
    "        for line in f:\n",
    "            tokens_ner_tags = json.loads(line)\n",
    "            sentence = \"\"\n",
    "            entity = \"\"\n",
    "            for item in tokens_ner_tags[\"tokens\"]:\n",
    "                sentence = sentence + item + \" \"\n",
    "            sentence_list.append(sentence)\n",
    "\n",
    "    with open(path_out, 'w', encoding='utf-8') as e:                \n",
    "        for sentence in sentence_list:\n",
    "            e.write(sentence + '\\n')   \n",
    "\n",
    "def get_entities_from_tokens_nertags(path_in, path_out):\n",
    "    #read the json file and change to dictionary\n",
    "    with open(path_in,'r', encoding='utf-8') as f:\n",
    "        tokens_tags = []\n",
    "        for line in f:\n",
    "            token_tag = json.loads(line) \n",
    "            tokens_tags.append(token_tag)\n",
    "    #tokens_tags = tokens_tags[:-1]  \n",
    "    with open(path_out,'w', encoding='utf-8') as f:\n",
    "        for line in tokens_tags:\n",
    "            words = \"\"\n",
    "            words_tags_list = \"\"\n",
    "            #words_tags_list = []\n",
    "            for index, tag in enumerate(line[\"ner_tags\"]):\n",
    "                # take entity, if it's not \"O\"\n",
    "                if tag != \"O\":\n",
    "                    if index > 0 and line[\"ner_tags\"][index] == line[\"ner_tags\"][index-1]: # index>0 -> index not out of range\n",
    "                        words = words + \" \" +  line[\"tokens\"][index]\n",
    "                    else:\n",
    "                        words = line[\"tokens\"][index]\n",
    "\n",
    "                    # the next tag different from the current one        \n",
    "                    if index < len(line[\"ner_tags\"])-1: # make sure index not out of range\n",
    "                        if line[\"ner_tags\"][index] != line[\"ner_tags\"][index+1] :\n",
    "                            words_tags_list =  words_tags_list    +   (words + \": \" + line[\"ner_tags\"][index]) + \"\\n\" \n",
    "                    \n",
    "                    # the last token is always full stop sign -> not even enter the if tag != \"O\":\n",
    "                    # thus, the next line of code is meaningless\n",
    "                    else:   \n",
    "                        words_tags_list = words_tags_list   +   (words + \": \" + line[\"ner_tags\"][index])  + \"\\n\"      \n",
    "            \n",
    "            if words_tags_list == \"\":\n",
    "                words_tags_list = \"no entity in this sentence \\n\"\n",
    "                \n",
    "            f.write(  (words_tags_list) + \"\\n\"  )             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06bd4f9",
   "metadata": {},
   "source": [
    "#### Get the original sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a14e4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = \"fabNER/fabner_simple_train.jsonl\"\n",
    "path_out = \"fabNER/fabner_simple_train_original_sentence.jsonl\"\n",
    "get_original_sentences_from_tokens_nertags(path_in, path_out)\n",
    "\n",
    "path_in = \"fabNER/fabner_simple_val.jsonl\"\n",
    "path_out = \"fabNER/fabner_simple_val_original_sentence.jsonl\"\n",
    "get_original_sentences_from_tokens_nertags(path_in, path_out)    \n",
    "\n",
    "path_in = \"fabNER/fabner_simple_test.jsonl\"\n",
    "path_out = \"fabNER/fabner_simple_test_original_sentence.jsonl\"\n",
    "get_original_sentences_from_tokens_nertags(path_in, path_out)    \n",
    "\n",
    "path_in = \"fabNER/fabner_simple_total.jsonl\"\n",
    "path_out = \"fabNER/fabner_simple_total_original_sentence.jsonl\"\n",
    "get_original_sentences_from_tokens_nertags(path_in, path_out)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda9686c",
   "metadata": {},
   "source": [
    "#### Get the ner_tags in ChatGPT format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44532ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = \"fabNER/fabner_simple_train.jsonl\"\n",
    "path_out = \"fabNER/fabner_simple_train_entities.jsonl\"\n",
    "get_entities_from_tokens_nertags(path_in, path_out)  \n",
    "\n",
    "path_in = \"fabNER/fabner_simple_val.jsonl\"\n",
    "path_out = \"fabNER/fabner_simple_val_entities.jsonl\"\n",
    "get_entities_from_tokens_nertags(path_in, path_out)  \n",
    "\n",
    "path_in = \"fabNER/fabner_simple_test.jsonl\"\n",
    "path_out = \"fabNER/fabner_simple_test_entities.jsonl\"\n",
    "get_entities_from_tokens_nertags(path_in, path_out) \n",
    "\n",
    "path_in = \"fabNER/fabner_simple_total.jsonl\"\n",
    "path_out = \"fabNER/fabner_simple_total_entities.jsonl\"\n",
    "get_entities_from_tokens_nertags(path_in, path_out) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849230b",
   "metadata": {},
   "source": [
    " #### Remember ro delete the last line of \"fabNER/fabner_simple_total_entities.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc0bba",
   "metadata": {},
   "source": [
    "# Assembly dataset - alternators, engines and gearboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c866ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_assembly(path):\n",
    "    data = pd.read_csv(path, sep = '\\t', header=None, names = [\"tokens\",\"ner_tags\"])\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8616a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assembly_train = read_files_assembly(\"Assembly-Stanford-NER/dataset/alternators-engines-gearboxes/dataset-A1-E1-G2-train-annotated.tsv\")\n",
    "#assembly_train, assembly_val = train_test_split( assembly_train, test_size=0.2, random_state=0)\n",
    "#assembly_test = read_files_assembly(\"Assembly-Stanford-NER/dataset/alternators-engines-gearboxes/dataset-A1-E1-G2-test-annotated.tsv\")\n",
    "\n",
    "assembly_train_aeg = read_files_assembly(r\"assembly_dataset/alternators-engines-gearboxes/dataset-A1-E1-G2-train-annotated.tsv\")\n",
    "assembly_test_aeg = read_files_assembly(r\"assembly_dataset/alternators-engines-gearboxes/dataset-A1-E1-G2-test-annotated.tsv\")\n",
    "assembly_a2 = read_files_assembly(r\"assembly_dataset/alternators/A2-Toyota-Alternator-Build-Manual/Toyota-Alternator-Build-Manual-annotated.tsv\")\n",
    "assembly_e2 = read_files_assembly(r\"assembly_dataset/engines/E2-HM1-Instruction-manual/HM1-Instruction-manual-annotated.tsv\")\n",
    "assembly_e3 = read_files_assembly(r\"assembly_dataset/engines/E3-SC1A-Instruction-manual/SC1A-Instruction-manual-annotated.tsv\")\n",
    "assembly_e4 = read_files_assembly(r\"assembly_dataset/engines/E4-TVR1ABB-Instruction-manual/TVR1ABB-Instruction-manual-annotated.tsv\")\n",
    "assembly_e5 = read_files_assembly(r\"assembly_dataset/engines/E5-TVR1A-Instruction-manual/TVR1A-Instruction-manual-annotated.tsv\")\n",
    "assembly_e6 = read_files_assembly(r\"assembly_dataset/engines/E6-VR1A-Instruction-manual/VR1A-Instruction-manual-annotated.tsv\")\n",
    "assembly_g2 = read_files_assembly(r\"assembly_dataset/gearboxes/G1-Assembly-instructions-for-gearbox/Assembly-instructions-for-gearbox-annotated.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aaab441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assembly_total\n",
    "#assembly_total = pd.concat([assembly_a2, assembly_e2, assembly_e3, assembly_e4, assembly_e5, assembly_e6, assembly_g2], ignore_index=True)\n",
    "assembly_total = pd.concat([assembly_train_aeg,assembly_test_aeg], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313f2335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DISASSEMBLY</td>\n",
       "      <td>OPER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AND</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BENCH</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHECKS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24342</th>\n",
       "      <td>Looking</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24343</th>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24344</th>\n",
       "      <td>any</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24345</th>\n",
       "      <td>leaks</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24346</th>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24347 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tokens ner_tags\n",
       "0      DISASSEMBLY     OPER\n",
       "1              AND        O\n",
       "2            BENCH        O\n",
       "3           CHECKS        O\n",
       "4             TEST        O\n",
       "...            ...      ...\n",
       "24342      Looking        O\n",
       "24343          for        O\n",
       "24344          any        O\n",
       "24345        leaks        O\n",
       "24346            0        O\n",
       "\n",
       "[24347 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembly_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aba8b3c",
   "metadata": {},
   "source": [
    "#### transform the pandas dataframe to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac9a49e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jsonl_assembly(df, dataset_path):\n",
    "    with open(dataset_path, 'w', encoding='utf-8') as f:\n",
    "        dic_seq = {}\n",
    "        token_list = []\n",
    "        ner_tags_list = []\n",
    "        for index, row in df.iterrows():\n",
    "            token_list.append(row[\"tokens\"])\n",
    "            ner_tags_list.append(row[\"ner_tags\"])\n",
    "            if row['tokens'] == '0':\n",
    "                # change \"0\" to \".\"\n",
    "                token_list[-1] = \".\"\n",
    "                dic_seq[\"tokens\"] = token_list\n",
    "                dic_seq[\"ner_tags\"] = ner_tags_list\n",
    "                f.write(json.dumps(dic_seq) + \"\\n\")\n",
    "                #json.dump(dic_seq, f, ensure_ascii=False)#, indent=1)\n",
    "                #print(dic_seq)\n",
    "                dic_seq = {}\n",
    "                token_list = []\n",
    "                ner_tags_list = []   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45bc6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_jsonl_assembly(df=assembly_total, dataset_path=\"assembly_dataset/assembly_total.jsonl\")\n",
    "# row 478, the last 2 entities need to be changed to  ->  \"WGT\", \"WGT\", \"O\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b905df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = \"assembly_dataset/assembly_total.jsonl\"\n",
    "path_out = \"assembly_dataset/assembly_total_original_sentence.jsonl\"\n",
    "get_original_sentences_from_tokens_nertags(path_in, path_out)\n",
    "\n",
    "path_out = \"assembly_dataset/assembly_total_entities.jsonl\"\n",
    "get_entities_from_tokens_nertags(path_in, path_out) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded58822",
   "metadata": {},
   "source": [
    " #### Remember ro delete the last line of \"assembly_dataset/assembly_total_entities.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf7d81",
   "metadata": {},
   "source": [
    "# Thin film head technology dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18ef943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jsonl_thin_film_technology_sentence_new_new(path_in, path_out, paragraph = False):\n",
    "\n",
    "    nltk.download('punkt') \n",
    "    with open(path_out, 'w', encoding='utf-8') as e:\n",
    "        for filename in filter(lambda p: p.endswith(\"txt\"), os.listdir(path_in)):\n",
    "            #initialization\n",
    "            files_txt = []\n",
    "            files_ann = []\n",
    "            files = {}\n",
    "\n",
    "            #for txt, get the list for all textual tokens\n",
    "            filepath = os.path.join(path_in, filename)\n",
    "            with open(filepath, mode='r',encoding='utf-8') as f:\n",
    "                files_txt = f.read()   \n",
    "                f.close()\n",
    "            files[\"tokens\"] = nltk.word_tokenize(files_txt)\n",
    "\n",
    "            #for ann, get the correct format for annotated dataset    \n",
    "            filename_ann = filename[:7] + \".ann\"\n",
    "            filepath_ann = os.path.join(path_in, filename_ann)\n",
    "            files_ann = pd.read_csv(filepath_ann, sep = '\\s+', header=None, \n",
    "                                    names = [\"T-id\",\"ner_tags\",\"span1\", \"span2\", \"tokens1\",\"tokens2\",\"tokens3\", \"tokens4\",\"tokens5\",\"tokens6\",  \n",
    "                                             \"tokens7\",\"tokens8\",\"tokens9\",\"tokens10\",\"tokens11\",\"tokens12\",\"tokens13\",\"tokens14\",\"tokens15\"], \n",
    "                                    engine='python')\n",
    "            files_ann = files_ann.replace([None, np.nan], \"\")\n",
    "            \n",
    "            # get the jsonl format that salmon can take as input\n",
    "            columns_token = ['tokens1','tokens2','tokens3','tokens4','tokens5','tokens6','tokens7','tokens8','tokens9',\n",
    "                                      \"tokens10\",\"tokens11\",\"tokens12\",\"tokens13\",\"tokens14\",\"tokens15\"]\n",
    "            ner_tags = []  \n",
    "            for index, single_token in enumerate(files['tokens']):\n",
    "                #for column in columns_token:   \n",
    "                for i in range(1,15):\n",
    "                    column = \"tokens\" + str(i)\n",
    "                    column2 = \"tokens\" + str(i+1)\n",
    "                    for tag_index, row in enumerate(files_ann[column]):\n",
    "                        row = row.strip(\".\")\n",
    "                        if index < len(files[\"tokens\"])-1: ## not reaching the last token of the original sentence \n",
    "                            if files_ann[column2][tag_index] != \"\":\n",
    "                                if single_token == row and files_ann[column2][tag_index] == files['tokens'][index+1]:\n",
    "                                    ner_tags.append(files_ann['ner_tags'][tag_index])  # assign the tag to ner_tags according to files_ann`\n",
    "                                    files_ann[column][tag_index] = \"\" # after finding the word in the original sentence, set it to '' to avoid double assignment\n",
    "                                    if ner_tags[-1] == \"Others\":\n",
    "                                        ner_tags[-1] = \"O\" \n",
    "                                    break\n",
    "                            else:  ## the second word of a phase of a certain entity is not empty, \n",
    "                                if single_token == row :  #not consider the second word in the condition\n",
    "                                    ner_tags.append(files_ann['ner_tags'][tag_index])  # assign the tag to ner_tags according to files_ann`\n",
    "                                    files_ann[column][tag_index] = \"\"\n",
    "                                    if ner_tags[-1] == \"Others\":\n",
    "                                        ner_tags[-1] = \"O\" \n",
    "                                    break    \n",
    "                        else: ## have reached the last token of the original sentence \n",
    "                            if single_token == row :  # only for the last one\n",
    "                                ner_tags.append(files_ann['ner_tags'][tag_index])  # assign the tag to ner_tags according to files_ann`\n",
    "                                files_ann[column][tag_index] = \"\"\n",
    "                                if ner_tags[-1] == \"Others\":\n",
    "                                    ner_tags[-1] = \"O\" \n",
    "                                break    \n",
    "                    if len(ner_tags) == index+1:    # if the word is found, do not go though all token columns\n",
    "                        break                         \n",
    "\n",
    "                if len(ner_tags) < index+1:    # if no equal string text is found in the columns of tokens\n",
    "                    ner_tags.append('O')\n",
    "            files['ner_tags'] = ner_tags\n",
    "\n",
    "            files = pd.DataFrame(files)\n",
    "            dic_seq = {}\n",
    "            token_list = []\n",
    "            ner_tags_list = []\n",
    "            if paragraph:\n",
    "                for index, row in files.iterrows():\n",
    "                    token_list.append(row[\"tokens\"])\n",
    "                    ner_tags_list.append(row[\"ner_tags\"])\n",
    "                dic_seq[\"tokens\"] = token_list\n",
    "                dic_seq[\"ner_tags\"] = ner_tags_list\n",
    "                e.write(json.dumps(dic_seq) + \"\\n\")\n",
    "                dic_seq = {}\n",
    "                token_list = []\n",
    "                ner_tags_list = []           \n",
    "                    \n",
    "            else:\n",
    "                for index, row in files.iterrows():\n",
    "                    token_list.append(row[\"tokens\"])\n",
    "                    ner_tags_list.append(row[\"ner_tags\"])\n",
    "                    if row['tokens'] == '.':\n",
    "                        dic_seq[\"tokens\"] = token_list\n",
    "                        dic_seq[\"ner_tags\"] = ner_tags_list\n",
    "                        e.write(json.dumps(dic_seq) + \"\\n\")\n",
    "                        dic_seq = {}\n",
    "                        token_list = []\n",
    "                        ner_tags_list = []         \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793483b",
   "metadata": {},
   "source": [
    "### sentence wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e5df3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_jsonl_thin_film_technology_sentence_new_new(path_in = \"thin-film-technology-dataset\", path_out = \"thin-film-technology-dataset/thin_film_head_technology_total.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "540ae9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = \"thin-film-technology-dataset/thin_film_head_technology_total.jsonl\"\n",
    "path_out = \"thin-film-technology-dataset/thin_film_head_technology_total_original_sentence.jsonl\"\n",
    "get_original_sentences_from_tokens_nertags(path_in, path_out)\n",
    "\n",
    "path_out = \"thin-film-technology-dataset/thin_film_head_technology_total_entities.jsonl\"\n",
    "get_entities_from_tokens_nertags(path_in, path_out) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e759f8",
   "metadata": {},
   "source": [
    "#### Remember ro delete the last line of \"thin-film-technology-dataset/thin_film_head_technology_total_entities_paragraph.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b96d908",
   "metadata": {},
   "source": [
    "### paragraph wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51cf1df2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\z004r5cc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "get_jsonl_thin_film_technology_sentence_new_new(path_in = \"thin-film-technology-dataset\", path_out = \"thin-film-technology-dataset/thin_film_head_technology_total_paragraph.jsonl\", paragraph = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef8bf637",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = \"thin-film-technology-dataset/thin_film_head_technology_total_paragraph.jsonl\"\n",
    "path_out = \"thin-film-technology-dataset/thin_film_head_technology_total_original_sentence_paragraph.jsonl\"\n",
    "get_original_sentences_from_tokens_nertags(path_in, path_out)\n",
    "\n",
    "path_out = \"thin-film-technology-dataset/thin_film_head_technology_total_entities_paragraph.jsonl\"\n",
    "get_entities_from_tokens_nertags(path_in, path_out) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78725fd",
   "metadata": {},
   "source": [
    " #### Remember ro delete the last line of \"thin-film-technology-dataset/thin_film_head_technology_total_entities_paragraph.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19f3f43c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\z004r5cc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#get_jsonl_thin_film_technology_sentence_new_new(path_in = \"thin-film-technology-dataset/thin-film-small\", path_out = \"thin-film-technology-dataset/thin-film-small/thin_film_technology_small.jsonl\")#, paragraph = True)\n",
    "#get_jsonl_thin_film_technology_version2_textblob(path_in = \"thin-film-technology-dataset/thin-film-small\", path_out = \"thin-film-technology-dataset/thin-film-small/thin_film_technology_small_textblob.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4c3958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd447ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95797dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
